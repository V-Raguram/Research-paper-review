**Efficient CNN Accelerator on FPGA** This paper presents a **Unified Blocked Winograd-GEMM** architecture implemented on a Xilinx Virtex-7 FPGA, targeting the **ResNet-18** neural network. The core innovation is the consolidation of two distinct convolution strategies—**Winograd Minimal Filtering** (optimized for small 3×3 filters) and **General Matrix Multiplication (GEMM)** (required for FC layers, 7×7 filters, and strided layers)—into a single shared Processing Element (PE) array. Unlike prior works that utilized separate hardware resources for these tasks, this design maximizes FPGA resource utilization by sharing the DSP blocks for both operations, using multiplexers to bypass the **Data/Output Transform Units (DTU/OTU)** during GEMM modes. The authors employ a **Blocking** strategy to decompose large feature map matrices into smaller tiles to fit within the limited on-chip **Block RAM (BRAM)** resources. By dynamically selecting optimal Winograd tile sizes (e.g., F(4,3) vs. F(2,3)) based on the specific memory constraints of each CNN layer, the design achieves a throughput of 383 GOPS at 200 MHz with 16-bit fixed-point precision, effectively trading off BRAM usage for computational speed.

Can do lot more computations but takes up lot of memory in the process . Bad for edge AI

3-7 ,10 15-19 ,24